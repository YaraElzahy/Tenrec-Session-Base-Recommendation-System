{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r35_eppo2-Rw"
      },
      "source": [
        "# CISC-839 Data Analytics Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1xBo13v2-Rz"
      },
      "source": [
        "## A Session Based - Recommendation System\n",
        "\n",
        "This System is a session-based recommendation system, powered by the Ternec dataset. Leveraging the [Tenrec: A Large-scale Multipurpose Benchmark Dataset for Recommender Systems](https://proceedings.neurips.cc/paper_files/paper/2022/file/4ad4fc1528374422dd7a69dea9e72948-Paper-Datasets_and_Benchmarks.pdf) Paper, we aim to deliver personalized recommendations that adapt to users' session-specific preferences in real-time. Join us as we revolutionize the way users discover content by harnessing the power of session-based modeling and the rich insights provided by the Ternec dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70koxp542-Rz"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGvdffcr2-R0"
      },
      "source": [
        "## Team Members\n",
        "- Adham Mokhtar\n",
        "- Manar El-Ghobashy\n",
        "- Yara Hassan\n",
        "- Yara El-Zahy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncneL4c62-R0"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwLLFCQl2-R1"
      },
      "source": [
        "## Some Explainations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7UoqwpG2-R1"
      },
      "source": [
        "A recommendation system is like having a smart computer friend that suggests things you might like based on the things you already enjoy. It helps you discover new things that you'll love!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gogGqV7u2-R1"
      },
      "source": [
        "There are three main types of recommendation systems:\n",
        "1. Content-Based: It recommends things based on the features of what you already like. For example, if you enjoy action movies, it will suggest more action movies.\n",
        "2. Collaborative Filtering: It recommends things based on what other people with similar tastes enjoy. If someone similar to you likes a certain video game, it will suggest that game to you.\n",
        "3. Hybrid: This combines different methods to give you even better recommendations. It uses both the features of what you like and what other people like to suggest things you'll enjoy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0CQANHF2-R1"
      },
      "source": [
        "\"Session-based\" and \"cold start\" are specific challenges or scenarios that can occur in recommendation systems:\n",
        "- Session-based Recommendations: This means giving recommendations based on what you're doing right now. For example, if you're browsing a website, it suggests things related to what you're looking at.\n",
        "- Cold Start Problem: This happens when the system doesn't know much about a new user or item. It's like when you join a new website or they add new things. The system has to find other ways to give you recommendations since it doesn't have much information yet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I80jBma82-R2"
      },
      "source": [
        "In this notebook (project) we will use collaborative filtering to try solving the session-based challenge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XZ4w27d2-R2"
      },
      "source": [
        "For session-based recommendations, a type of recommendation system called \"Sequential Recommendation\" is more suitable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8i1F9QM2-R2"
      },
      "source": [
        "- Sequential Recommendation systems are specifically designed to make recommendations based on a user's current session or sequence of actions within a single browsing session. These systems analyze the order and context of the user's interactions to understand their immediate preferences and provide relevant suggestions.\n",
        "- In a session-based recommendation system, the focus is on capturing the user's current interests and recommending items that align with those interests. This type of system takes into account the sequence of actions, such as clicks, views, or purchases, during a session to make accurate and timely recommendations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64VKMVXP2-R3"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this notebook, I am experimenting with the NextItNet model. Each member of the team is trying a different model for comparison and evaluation. I'll focus on training, evaluating, and analyzing the NextItNet model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW_AzokE2-R3"
      },
      "source": [
        "## Recommended Pipline\n",
        "- Data Preprocessing\n",
        "- Session Representation\n",
        "- Train-Test-Validation Split\n",
        "- Model Selection\n",
        "    - Model Training\n",
        "    - Model Evaluation\n",
        "    - Hyperparameter Tuning\n",
        "    - Performance Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY7MQUlS2-R3"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2lt11vw2-R3"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Fphn0FjI2-R3"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Importing the NextItNet recommendation model\n",
        "from model.nextitnet import NextItNet\n",
        "\n",
        "# Importing the SummaryWriter from PyTorch for TensorBoard logging\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Importing other utility functions and modules\n",
        "import random\n",
        "from utils import trainer, dataset\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "# Importing libraries for graph plotting\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3KgR_xc62-R4"
      },
      "outputs": [],
      "source": [
        "# Set the random seed to 22\n",
        "seed = 22\n",
        "\n",
        "# Seed the random number generator for Python's built-in 'random' module\n",
        "random.seed(seed)\n",
        "\n",
        "# Set the seed for the hash value generated by Python's hash function (used for dictionaries, sets, etc.)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "# Seed the random number generator for NumPy library\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Seed the random number generator for PyTorch on the CPU\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Seed the random number generator for PyTorch on the GPU (CUDA)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Turn off GPU-specific optimizations that may introduce variability in the results\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a SummaryWriter instance for logging with TensorBoard\n",
        "writer = SummaryWriter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX8elFB62-R4"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEZhHy9Y2-R5"
      },
      "source": [
        "## Data Acquisition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIma63Kb2-R5"
      },
      "source": [
        "The four datasets used in this paper are raw datasets:\n",
        "- `QK-video.csv`\n",
        "- `QK-article.csv`\n",
        "- `QB-video.csv`\n",
        "- `QB-article.csv`\n",
        "\n",
        "All task-specific datasets are extracted from these datasets.\n",
        "\n",
        "Subdataset:\n",
        "\n",
        "1. `ctr_data_1M.csv` is used in CTR task (Section 3.1) and Multi-Task Learning  (Section 3.3).\n",
        "\n",
        "2. `cold_data.csv` (Table 7, Section 3.6), `cold_data_1.csv`, `cold_data_0.3.csv`, `cold_data_0.7.csv` are used for the Cold-Start task (see Appendix Table 5).\n",
        "\n",
        "3. `task_0.csv`, `task_1.csv`, `task_2.csv`, `task_3.csv` are used in Lifelong Learning (see Section 3.7, Table 8).\n",
        "\n",
        "4. `sbr_data_1M.csv` is used in Session-based Recommendation (Section 3.2), Transfer Learning (Section 3.4, used as pre-training dataset), User Profile Prediction (Section 3.5), Model Compression (Section 3.8), Model Training Speedup (Section 3.9).\n",
        "\n",
        "Note that:\n",
        "\n",
        "1. Model Inference Speedup Task (Section 3.10): the dataset is `QB-video.csv`, and Transfer Learning Task (Section 3.4): target dataset is also `QB-video.csv`.\n",
        "\n",
        "2. We sort the items at the user level in order of click time, so the time information is implicit in the order of the items.\n",
        "\n",
        "Example:\n",
        "| userid | itemid |\n",
        "| ---    | ---    |\n",
        "| 2345   | 12     |\n",
        "| 2345   | 5      |\n",
        "| 2345   | 61     |\n",
        "| 2345   | 78     |\n",
        "| 2345   | 35     |\n",
        "\n",
        "The click sequence of user 2345 is [12, 5, 61, 78, 35].\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkXksvE12-R5"
      },
      "source": [
        "We report baseline results evaluated on QK-video-1M here. Results of the full QK-video\n",
        "datasets will be present in the leaderboard. Following the common practice [61], we simply filter\n",
        "out sessions with length shorter than 10. Given that the average session length is 28.34, we set the\n",
        "maximum session lengths to 30. Session length less than 30 will be padded with zero, otherwise only\n",
        "recent 30 interactions are kept. After pre-processing, we obtain 928,562 users, 1,189,341 items and\n",
        "37,823,609 clicking interactions. We keep the last item in the session for testing, the second to last\n",
        "for validating, and the remaining for training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyOPaMtM2-R5"
      },
      "source": [
        "So we will use `sbr_data_1M.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwR6MYSN2-R5",
        "outputId": "e48c3fc8-b72c-48bb-ec34-1af54b96c897"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\adham\\AppData\\Local\\Temp\\ipykernel_13264\\4111369473.py:2: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  data = pd.read_csv('sbr_data_1M.csv')\n"
          ]
        }
      ],
      "source": [
        "# Read data from the CSV file 'sbr_data_1M.csv' and store it in a DataFrame\n",
        "data = pd.read_csv('sbr_data_1M.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Kyk5Hll92-R5",
        "outputId": "15f29410-0d01-461a-a954-ea0d576f9b1d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>click</th>\n",
              "      <th>follow</th>\n",
              "      <th>like</th>\n",
              "      <th>share</th>\n",
              "      <th>video_category</th>\n",
              "      <th>watching_times</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>80936</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>781</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   user_id  item_id  click  follow  like  share video_category  \\\n",
              "0        1        1      1       0     0      0              1   \n",
              "1        1        2      1       0     0      0              1   \n",
              "2        1        3      1       0     0      0              0   \n",
              "3        1    80936      1       0     0      0              1   \n",
              "4        1      781      1       0     0      0              1   \n",
              "\n",
              "   watching_times  gender  age  \n",
              "0               1       1    4  \n",
              "1               1       1    4  \n",
              "2               1       1    4  \n",
              "3               1       1    4  \n",
              "4               1       1    4  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display the first 5 rows of the DataFrame 'data'\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EU6lYjf02-R6",
        "outputId": "8f765546-ad20-4658-9d52-acfe824da6c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 38300254 entries, 0 to 38300253\n",
            "Data columns (total 10 columns):\n",
            " #   Column          Dtype \n",
            "---  ------          ----- \n",
            " 0   user_id         int64 \n",
            " 1   item_id         int64 \n",
            " 2   click           int64 \n",
            " 3   follow          int64 \n",
            " 4   like            int64 \n",
            " 5   share           int64 \n",
            " 6   video_category  object\n",
            " 7   watching_times  int64 \n",
            " 8   gender          int64 \n",
            " 9   age             int64 \n",
            "dtypes: int64(9), object(1)\n",
            "memory usage: 2.9+ GB\n"
          ]
        }
      ],
      "source": [
        "# Display a concise summary of the DataFrame 'data'\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT_ls-ZP2-R6"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye6TqlTE2-R6"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset contains duplicates, but since we are dealing with sequences, we cannot simply drop these duplicates, as it could significantly impact the sequence integrity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Missing Value Handling\n",
        "Data doesn't contain missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "user_id           0\n",
              "item_id           0\n",
              "click             0\n",
              "follow            0\n",
              "like              0\n",
              "share             0\n",
              "video_category    0\n",
              "watching_times    0\n",
              "gender            0\n",
              "age               0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculate the number of missing values in each column.\n",
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dropping columns\n",
        "\n",
        "#### 1. Session-based Recommendation System:\n",
        "    The code is preparing the data for a session-based recommendation system. In this context, the focus is on predicting user-item interactions within a session, and the other columns such as 'click', 'follow', 'like', and 'share' might not be relevant for session-level recommendations.\n",
        "\n",
        "#### 2. Model Training:\n",
        "    The code intends to train the recommendation model using only the 'user_id' and 'item_id' columns. These columns represent the essential information required for session-based recommendation, where the goal is to predict which items a user might interact with in the current session.\n",
        "\n",
        "#### 3. Filtering Data Based on Age:\n",
        "    The 'age' column is retained in the DataFrame because it is used for filtering the data. By keeping the 'age' column, the code ensures that only data points corresponding to a specific age group are considered for the session-based recommendation task. It helps tailor the recommendation system to cater to specific user age groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop specified columns from the DataFrame 'data'\n",
        "data.drop(columns=['click', 'follow', 'like', 'share', 'video_category', 'watching_times', 'gender'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO6adL6H2-R8"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSw0qdW22-R8"
      },
      "source": [
        "## Data Analysis (ERD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OkrTNSl2-R8"
      },
      "source": [
        "### Age Categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "Rs9AGOhM2-R8",
        "outputId": "547922d9-e2f4-4888-b0dc-a1343507528f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "data['age'].value_counts().plot.bar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count the number of unique users with age = 3 in the dataset\n",
        "age_3_users_count = data[data['age'] == 3]['user_id'].nunique()\n",
        "\n",
        "# Count the number of unique users with age = 0 in the dataset\n",
        "age_0_users_count = data[data['age'] == 0]['user_id'].nunique()\n",
        "\n",
        "# Count the number of unique items associated with users with age = 3 in the dataset\n",
        "age_3_items_count = data[data['age'] == 3]['item_id'].nunique()\n",
        "\n",
        "# Count the number of unique items associated with users with age = 0 in the dataset\n",
        "age_0_items_count = data[data['age'] == 0]['item_id'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of users when age = 3 : 216460\n",
            "Number of users when age = 0 : 189218\n",
            "Number of items when age = 3 : 628684\n",
            "Number of items when age = 0 : 605126\n"
          ]
        }
      ],
      "source": [
        "print(f'Number of users when age = 3 : {age_3_users_count}')\n",
        "print(f'Number of users when age = 0 : {age_0_users_count}')\n",
        "print(f'Number of items when age = 3 : {age_3_items_count}')\n",
        "print(f'Number of items when age = 0 : {age_0_items_count}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Collaborative Filtering\n",
        "\n",
        "Ways to determine whether user-based or item-based collaborative filtering is more suitable:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- User-based filtering is better when you have a large number of users with consistent preferences and behaviors, and when users have interacted with a significant portion of the available items.\n",
        "- Item-based filtering is better when you have a large number of items with consistent characteristics and interactions, and when users have interacted with only a small fraction of the available items."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the number of items in each dataset is larger than the number of users, an Item-Based filtering approach appears to be more suitable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- We are preparing a subset of the data for testing the generalization of the session-based recommendation model.\n",
        "- We select users with age values of 0 and 3, as they have similar record counts, user counts, and item counts to be away of data bias.\n",
        "- Due to GPU resource limitations and previous training failures on Colab and the university server, we are restricting the dataset to one million records\n",
        "- This filtered data will be used to evaluate the model's performance and assess if it can maintain a high value of NDCG@20 (Normalized Discounted Cumulative Gain at top-20) on this combination of age groups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data for experiment 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>11</td>\n",
              "      <td>141</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>11</td>\n",
              "      <td>142</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>11</td>\n",
              "      <td>143</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>441</th>\n",
              "      <td>11</td>\n",
              "      <td>144</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>442</th>\n",
              "      <td>11</td>\n",
              "      <td>145</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2677809</th>\n",
              "      <td>58161</td>\n",
              "      <td>6900</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2677810</th>\n",
              "      <td>58161</td>\n",
              "      <td>5736</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2677811</th>\n",
              "      <td>58161</td>\n",
              "      <td>6083</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2677812</th>\n",
              "      <td>58161</td>\n",
              "      <td>28563</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2677813</th>\n",
              "      <td>58161</td>\n",
              "      <td>11572</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         user_id  item_id  age\n",
              "438           11      141    3\n",
              "439           11      142    3\n",
              "440           11      143    3\n",
              "441           11      144    3\n",
              "442           11      145    3\n",
              "...          ...      ...  ...\n",
              "2677809    58161     6900    3\n",
              "2677810    58161     5736    3\n",
              "2677811    58161     6083    3\n",
              "2677812    58161    28563    3\n",
              "2677813    58161    11572    3\n",
              "\n",
              "[1000000 rows x 3 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Select users with age = 3 from the original data and store it in 'age1'.\n",
        "age1 = data[data['age'] == 3]\n",
        "\n",
        "# Limit the 'age1' DataFrame to the first 1 million rows.\n",
        "age1 = age1[0:1000000]\n",
        "\n",
        "# Display the resulting DataFrame 'age1'\n",
        "age1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data for experiment 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>247</th>\n",
              "      <td>8</td>\n",
              "      <td>97</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248</th>\n",
              "      <td>8</td>\n",
              "      <td>98</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249</th>\n",
              "      <td>8</td>\n",
              "      <td>99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250</th>\n",
              "      <td>8</td>\n",
              "      <td>106</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>8</td>\n",
              "      <td>400846</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6065303</th>\n",
              "      <td>133559</td>\n",
              "      <td>260</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6065304</th>\n",
              "      <td>133559</td>\n",
              "      <td>4850</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6065305</th>\n",
              "      <td>133559</td>\n",
              "      <td>206700</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6065306</th>\n",
              "      <td>133559</td>\n",
              "      <td>327063</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6065307</th>\n",
              "      <td>133559</td>\n",
              "      <td>14757</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         user_id  item_id  age\n",
              "247            8       97    0\n",
              "248            8       98    0\n",
              "249            8       99    0\n",
              "250            8      106    0\n",
              "251            8   400846    0\n",
              "...          ...      ...  ...\n",
              "6065303   133559      260    0\n",
              "6065304   133559     4850    0\n",
              "6065305   133559   206700    0\n",
              "6065306   133559   327063    0\n",
              "6065307   133559    14757    0\n",
              "\n",
              "[1000000 rows x 3 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Select users with age = 0 from the original data and store it in 'age2'.\n",
        "age2 = data[data['age'] == 0]\n",
        "\n",
        "# Limit the 'age2' DataFrame to the first 1 million rows.\n",
        "age2 = age2[0:1000000]\n",
        "\n",
        "# Display the resulting DataFrame 'age2'\n",
        "age2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Free up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Ta1xXSnAz8df"
      },
      "outputs": [],
      "source": [
        "# Delete the variable 'data' from memory to free up resources.\n",
        "# This step is performed to ensure efficient memory management, especially when working with large datasets.\n",
        "# Since we have already created a subset of the data in 'result_data' for the current analysis,\n",
        "# we can safely delete the original DataFrame 'data' to release the memory it occupied.\n",
        "\n",
        "del data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4KH5HQDlSDb"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1CfH9-hlSDb"
      },
      "source": [
        "# Experiment 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP7u7Izt2-R-"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sessions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Number of items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvFARK102-R-",
        "outputId": "d07e10d9-310c-42c4-8bc4-4b69ac9e52cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "201148"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculate the number of unique items\n",
        "num_items1 = age1['item_id'].nunique()\n",
        "\n",
        "# Display the number of unique items\n",
        "num_items1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Number of users"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X87IfLjs2-R-",
        "outputId": "6c75b68c-a691-429f-aaac-e979d8a77408"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20686"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculate the number of unique users\n",
        "num_users1 = age1['user_id'].nunique()\n",
        "\n",
        "# Display the number of unique users\n",
        "num_users1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2RSbrj32-SA"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDFRptVX2-SA"
      },
      "source": [
        "## Session Representation\n",
        "- Items Label Encoding\n",
        "- Session Initialization\n",
        "- Session Length Filtering\n",
        "- Session Sorting - already sorted\n",
        "- Session Padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Label Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The reason for using label encoding is that the 'item_id' column contains gaps between these IDs.\n",
        "- Label encoding is a suitable choice in this case to represent the 'item_id' values in a compact and sequential manner.\n",
        "- This approach helps in making the data representation more realistic, and it is particularly useful when working with categorical or nominal data where there is no intrinsic ordinal relationship between the categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "M0hvx8qv2-SA"
      },
      "outputs": [],
      "source": [
        "# Create a label encoding mapping for the item ids\n",
        "# The label encoding will assign unique integers to each unique 'item_id', starting from 1\n",
        "itemIdMapping1 = {k:i+1 for i, k in enumerate(sorted(list(age1['item_id'].unique())))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "zTluGfcI2-SA"
      },
      "outputs": [],
      "source": [
        "# Create a new column 'item_id_mapped' by mapping the original 'item_id' values to their label encoded equivalents.\n",
        "age1[\"item_id_mapped\"] = age1['item_id'].map(itemIdMapping1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "LvgCr2p72-SA",
        "outputId": "60980103-030b-43e5-9209-f9a1efec56a6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>age</th>\n",
              "      <th>item_id_mapped</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>11</td>\n",
              "      <td>141</td>\n",
              "      <td>3</td>\n",
              "      <td>105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>11</td>\n",
              "      <td>142</td>\n",
              "      <td>3</td>\n",
              "      <td>106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>11</td>\n",
              "      <td>143</td>\n",
              "      <td>3</td>\n",
              "      <td>107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>441</th>\n",
              "      <td>11</td>\n",
              "      <td>144</td>\n",
              "      <td>3</td>\n",
              "      <td>108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>442</th>\n",
              "      <td>11</td>\n",
              "      <td>145</td>\n",
              "      <td>3</td>\n",
              "      <td>109</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     user_id  item_id  age  item_id_mapped\n",
              "438       11      141    3             105\n",
              "439       11      142    3             106\n",
              "440       11      143    3             107\n",
              "441       11      144    3             108\n",
              "442       11      145    3             109"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display the first 5 rows of the DataFrame 'age1'\n",
        "age1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg3HRJlc2-SA"
      },
      "source": [
        "### Session Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- This operation effectively organizes the data into sessions based on user interactions.\n",
        "- Each session represents a sequence of items (represented by their label encoded integers) that a user has interacted with, and these sequences are grouped according to each user's session.\n",
        "- The 'sessions' DataFrame will be useful for building and training session-based recommendation models that leverage the sequential patterns of user-item interactions for personalized recommendations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "7empP3RZ2-SA"
      },
      "outputs": [],
      "source": [
        "# Group the DataFrame 'age1' by 'user_id' and aggregate the corresponding 'item_id_mapped' values into lists.\n",
        "# Each session corresponds to a user, and the 'item_id_mapped' values for that user are collected in a list.\n",
        "# The resulting DataFrame 'sessions' will have each user_id as the index and a list of corresponding 'item_id_mapped' values as the 'item_id_mapped' column.\n",
        "\n",
        "sessions1 = age1.groupby(\"user_id\")[['item_id_mapped']].agg(list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "u2I2zwlp2-SA",
        "outputId": "6957502f-355c-4bfa-9b05-b33e05849e08"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item_id_mapped</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>user_id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[105, 106, 107, 108, 109, 110, 111, 112, 113, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>[154, 155, 156, 157, 159, 160, 161, 2696, 2880...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>[190, 191, 192, 193, 194, 195, 196, 199, 203, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>[293, 294, 296, 299, 300, 39172, 16505, 29258,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>[399, 400, 401, 402, 853, 7854, 5935, 5629, 31...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            item_id_mapped\n",
              "user_id                                                   \n",
              "11       [105, 106, 107, 108, 109, 110, 111, 112, 113, ...\n",
              "14       [154, 155, 156, 157, 159, 160, 161, 2696, 2880...\n",
              "17       [190, 191, 192, 193, 194, 195, 196, 199, 203, ...\n",
              "29       [293, 294, 296, 299, 300, 39172, 16505, 29258,...\n",
              "39       [399, 400, 401, 402, 853, 7854, 5935, 5629, 31..."
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display the first 5 rows of the DataFrame 'sessions'\n",
        "sessions1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DUJD13V2-SA"
      },
      "source": [
        "### Session Length Filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "AnZqt9dr2-SB"
      },
      "outputs": [],
      "source": [
        "# Define the minimum and maximum sequence lengths for session-based recommendation based on the paper.\n",
        "\n",
        "# Minimum Sequence Length:\n",
        "# The 'min_sequence_length' variable is set to 10, which specifies the minimum number of items required in a session (user's interaction sequence).\n",
        "# Sessions with fewer than 10 items may be excluded from the analysis to ensure that sessions used for training have a minimum length to capture meaningful patterns.\n",
        "\n",
        "min_sequence_length = 10\n",
        "\n",
        "# Maximum Sequence Length:\n",
        "# The 'max_sequence_length' variable is set to 30, which represents the maximum allowed number of items in a session (user's interaction sequence).\n",
        "# Sessions with more than 30 items may be truncated to this maximum length to avoid excessively long sequences, ensuring computational efficiency during training.\n",
        "\n",
        "max_sequence_length = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "5sZNpJbR2-SB"
      },
      "outputs": [],
      "source": [
        "# Filter the 'sessions' DataFrame to include only sessions with a length greater than the 10.\n",
        "sessions1 = sessions1[sessions1['item_id_mapped'].apply(lambda x: len(x) > min_sequence_length)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpG1Q9sH2-SB",
        "outputId": "1d0f6ef2-6a47-482c-d148-a2f65040e47f"
      },
      "outputs": [],
      "source": [
        "# Filter the 'sessions' DataFrame to include only sessions with a length less than the 30.\n",
        "sessions1['item_id_mapped'] = sessions1['item_id_mapped'].apply(lambda x: x[:max_sequence_length])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXTbFHsW2-SB",
        "outputId": "2cd58987-8e57-4482-de4e-9c1cc06a0e9b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([list([105, 106, 107, 108, 109, 110, 111, 112, 113, 115, 116, 117, 118, 121, 122, 123, 641, 812, 3625, 95199, 3489, 9135, 573, 10571, 16411, 1435, 10684, 5678, 3661, 1071]),\n",
              "       list([154, 155, 156, 157, 159, 160, 161, 2696, 28807, 25609, 1610, 984, 5379, 83607, 18496, 9400, 2912, 3711, 13524, 25979, 18271, 52759, 5809, 14364, 65685, 17853, 1614, 7293, 12061, 12746]),\n",
              "       list([190, 191, 192, 193, 194, 195, 196, 199, 203, 204, 642, 1275, 11121, 639, 99589, 3659, 5382, 1524, 583, 7546, 3419, 13694, 1812, 622, 11208, 32862, 2736, 5584, 2288, 45926]),\n",
              "       ...,\n",
              "       list([77954, 1611, 10413, 53013, 14340, 83841, 405, 1019, 2456, 12612, 11456, 5469, 9912]),\n",
              "       list([47233, 1201, 8907, 1029, 1166, 83842, 5611, 20458, 65352, 22211, 6450, 45613, 6595, 644, 758, 40977, 129640, 9557, 122895, 79853, 100346, 91018, 3543, 2642, 2585, 9221, 54279, 662, 26711, 44978]),\n",
              "       list([18526, 1549, 48120, 840, 621, 1830, 72040, 97, 992, 1588, 5221, 4339, 4602, 19608, 8555])],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display the 'item_id_mapped' values.\n",
        "sessions1['item_id_mapped'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzBZfu5p2-SB"
      },
      "source": [
        "### Session Padding\n",
        "add padding to the beginning of each sequance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9-k3OUZ2-SB",
        "outputId": "aae7e427-9b6a-48d9-efb3-c099f68a8296"
      },
      "outputs": [],
      "source": [
        "# Pad the 'item_id_mapped' lists with zeros to make each sequence length equal to the defined maximum sequence length.\n",
        "sessions1['item_id_mapped'] = sessions1['item_id_mapped'].apply(lambda x: [0] * (max_sequence_length - len(x)) + x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "t2fyxhKS2-SB",
        "outputId": "2c82e247-e0d6-44b3-f0a4-70ff47eff680"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item_id_mapped</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>user_id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[105, 106, 107, 108, 109, 110, 111, 112, 113, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>[154, 155, 156, 157, 159, 160, 161, 2696, 2880...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>[190, 191, 192, 193, 194, 195, 196, 199, 203, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>[0, 0, 0, 0, 293, 294, 296, 299, 300, 39172, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>[399, 400, 401, 402, 853, 7854, 5935, 5629, 31...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            item_id_mapped\n",
              "user_id                                                   \n",
              "11       [105, 106, 107, 108, 109, 110, 111, 112, 113, ...\n",
              "14       [154, 155, 156, 157, 159, 160, 161, 2696, 2880...\n",
              "17       [190, 191, 192, 193, 194, 195, 196, 199, 203, ...\n",
              "29       [0, 0, 0, 0, 293, 294, 296, 299, 300, 39172, 1...\n",
              "39       [399, 400, 401, 402, 853, 7854, 5935, 5629, 31..."
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display the first 5 rows of the DataFrame 'sessions'\n",
        "sessions1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Common Argument"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the 'args' Namespace to store configuration and hyperparameters for the session-based recommendation model training.\n",
        "\n",
        "args1 = argparse.Namespace(\n",
        "    seed=seed,                             # Seed for reproducibility\n",
        "    save_path='./checkpoint/',             # Directory to save model checkpoints\n",
        "    model_name='NextItNet1',               # Name of the best model checkpoint\n",
        "\n",
        "    device='cuda',                         # Device to use for training ('cuda' for GPU, 'cpu' for CPU)\n",
        "\n",
        "    is_parallel=False,                     # Flag to enable/disable model parallelism (multi-GPU training)\n",
        "\n",
        "    # get_data arguments\n",
        "    item_min=min_sequence_length,          # Minimum sequence length for a session\n",
        "    valid_rate=100,                        # Ratio of data samples to use for validation\n",
        "    max_len=max_sequence_length,           # Maximum allowed sequence length for a session\n",
        "    bert_mask_prob=0.3,                    # Probability for applying BERT-style masking to sequence data\n",
        "    pad_token=0,                           # Token value used for padding sequences\n",
        "    train_batch_size=32,                   # Batch size for training data\n",
        "    test_batch_size=32,                    # Batch size for testing data\n",
        "    val_batch_size=32,                     # Batch size for validation data\n",
        "\n",
        "    # load_model arguments\n",
        "    embedding_size=128,                   # Size of the item embedding vector\n",
        "    block_num=2,                          # Number of blocks in the model\n",
        "    dilations=[1, 4],                     # Dilation rates for the blocks\n",
        "    kernel_size=3,                        # Size of the convolutional kernel\n",
        "    num_items=num_items1,                  # Number of unique items in the dataset\n",
        "    num_users=num_users1,                  # Number of unique users in the dataset\n",
        "    num_heads=4,                          # Number of attention heads in the transformer\n",
        "    dropout=0.1,                          # Dropout rate for regularization\n",
        "\n",
        "    # SeqTrain arguments\n",
        "    epochs=20,                            # Number of training epochs\n",
        "    is_pretrain=1,                        # Flag for pretraining mode (0: pretrain, 1: train from scratch)\n",
        "    lr=0.0001,                            # Learning rate for optimization\n",
        "    weight_decay=0.0,                     # Weight decay regularization parameter\n",
        "    local_rank=None,                      # Local rank for distributed training\n",
        "    metric_ks=[1, 5, 10, 20],             # Values of 'k' for evaluating top-k metrics (e.g., top-1, top-5, etc.)\n",
        "    hidden_size=16,                       # Size of the hidden layer in the NextItNet model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the device\n",
        "device = torch.device(args1.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-B2alrG2-SB"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW3nUOsq2-SB"
      },
      "source": [
        "## Train-Test-Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jjX2qoL2-SB",
        "outputId": "f7b3d61f-f618-404c-d9d6-33c2fd6699e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19960/19960 [00:00<00:00, 759384.17it/s]\n"
          ]
        }
      ],
      "source": [
        "# Split the session data into training, validation, and test sets using train_val_test_split function.\n",
        "train_data1, val_data1, test_data1 = dataset.train_val_test_split(sessions1['item_id_mapped'].apply(list).to_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the train_data and val_data dictionaries into smaller subsets for session-based recommendation model validation.\n",
        "\n",
        "# Initialize empty dictionaries to store the smaller train and validation data subsets.\n",
        "train_data_s1, val_data_s1 = {}, {}\n",
        "\n",
        "# Get the total number of data samples in the train_data dictionary.\n",
        "data_len1 = len(train_data1)\n",
        "\n",
        "# Set up a loop to iterate over the keys (sessions) in the val_data dictionary.\n",
        "# The loop copies the session data from the original train_data and val_data dictionaries into the smaller subsets train_data_s and val_data_s.\n",
        "# The loop stops when a fraction of the data (specified by args.valid_rate) has been copied into the validation subset.\n",
        "\n",
        "i = 0\n",
        "for key, _ in val_data1.items():\n",
        "    # Copy the session data from train_data into train_data_s.\n",
        "    train_data_s1[key] = train_data1[key]\n",
        "\n",
        "    # Copy the session data from val_data into val_data_s.\n",
        "    val_data_s1[key] = val_data1[key]\n",
        "\n",
        "    # Increment the counter 'i' to track the number of iterations.\n",
        "    i += 1\n",
        "\n",
        "    # Check if the number of iterations has reached the fraction of the total data specified by args.valid_rate.\n",
        "    # If it has, stop the loop to limit the size of the validation subset.\n",
        "    if i == int(data_len1 / args1.valid_rate):\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "-TIs3qSy2-SC"
      },
      "outputs": [],
      "source": [
        "# Create three datasets for session-based recommendation model training and evaluation.\n",
        "\n",
        "# 1. train_dataset:\n",
        "# The train_dataset is built using the BuildTrainDataset class, which constructs a training dataset from the train_data dictionary.\n",
        "# The training dataset includes input sequences (sessions) from train_data with a maximum length of args.max_len, padded with args.pad_token to reach the maximum length.\n",
        "# args.num_items represents the number of unique items in the dataset, which is used for encoding the item IDs.\n",
        "\n",
        "train_dataset1 = dataset.BuildTrainDataset(train_data1, args1.max_len, args1.pad_token, args1.num_items)\n",
        "\n",
        "# 2. valid_dataset:\n",
        "# The valid_dataset is constructed using the Build_full_EvalDataset class, which builds a full evaluation dataset from the train_data_s and val_data_s dictionaries.\n",
        "# The full evaluation dataset combines training and validation data, allowing evaluation on both datasets during model training.\n",
        "# The input sequences (sessions) from both train_data_s and val_data_s have a maximum length of args.max_len and are padded with args.pad_token to reach the maximum length.\n",
        "# args.num_items represents the number of unique items in the dataset, which is used for encoding the item IDs.\n",
        "\n",
        "valid_dataset1 = dataset.Build_full_EvalDataset(train_data_s1, val_data_s1, args1.max_len, args1.pad_token, args1.num_items)\n",
        "\n",
        "# 3. test_dataset:\n",
        "# The test_dataset is created using the Build_full_EvalDataset class, which builds a full evaluation dataset from the train_data and test_data dictionaries.\n",
        "# Similar to the valid_dataset, the full evaluation dataset combines training and testing data to allow evaluation on both datasets after model training.\n",
        "# The input sequences (sessions) from both train_data and test_data have a maximum length of args.max_len and are padded with args.pad_token to reach the maximum length.\n",
        "# args.num_items represents the number of unique items in the dataset, which is used for encoding the item IDs.\n",
        "\n",
        "test_dataset1 = dataset.Build_full_EvalDataset(train_data1, test_data1, args1.max_len, args1.pad_token, args1.num_items)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "TvgVeMBP2-SC"
      },
      "outputs": [],
      "source": [
        "# Create three data loaders for the session-based recommendation model training and evaluation.\n",
        "\n",
        "# 1. train_dataloader:\n",
        "# The train_dataloader is created using the get_train_loader function, which generates a data loader for the train_dataset.\n",
        "# The train_dataset contains training samples that are organized in batches to facilitate efficient model training.\n",
        "# The get_train_loader function handles batch creation, taking into account the training batch size specified in args.train_batch_size.\n",
        "\n",
        "train_dataloader1 = dataset.get_train_loader(train_dataset1, args1)\n",
        "\n",
        "# 2. valid_dataloader:\n",
        "# The valid_dataloader is generated using the get_val_loader function, which creates a data loader for the valid_dataset.\n",
        "# The valid_dataset includes a combination of training and validation data, used for model validation during training.\n",
        "# Similar to the train_dataloader, the get_val_loader function manages batch creation, considering the validation batch size specified in args.val_batch_size.\n",
        "\n",
        "valid_dataloader1 = dataset.get_val_loader(valid_dataset1, args1)\n",
        "\n",
        "# 3. test_dataloader:\n",
        "# The test_dataloader is constructed using the get_test_loader function, which generates a data loader for the test_dataset.\n",
        "# The test_dataset combines training and testing data, facilitating model evaluation on both datasets after training.\n",
        "# Like the previous dataloaders, the get_test_loader function handles batch creation, considering the test batch size specified in args.test_batch_size.\n",
        "\n",
        "test_dataloader1 = dataset.get_test_loader(test_dataset1, args1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Selection\n",
        "\n",
        "Steps for Session-Based Recommendation:\n",
        "\n",
        "- Research and Understanding: Familiarize ourself with the characteristics, strengths, and limitations of each model considered for session-based recommendation, namely BERT4Rec, GRU4Rec, SASRec, and NextItNet. Gain insights into their architectures, training approaches.\n",
        "\n",
        "- Evaluation Metrics: Determine the evaluation metrics that align with your project goals, such as recall and Normalized Discounted Cumulative Gain (NDCG). Ensure that the selected models can be evaluated using these metrics.\n",
        "\n",
        "- Implementation Suitability: Assess the feasibility and compatibility of each model with the dataset, session-based recommendation task, and available computational resources.\n",
        "\n",
        "- Robustness and Generalization: Analyze the robustness and generalization capabilities of each model by assessing their performance across different subsets for example different ages groups as we did. Consider their ability to handle various session lengths we tried different values of session min/max sequence length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NextItNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk6O3uCp2-SC"
      },
      "source": [
        "#### Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "sdC3wpHu2-SD"
      },
      "outputs": [],
      "source": [
        "# Instantiate the NextItNet model with the provided configuration and hyperparameters.\n",
        "model1 = NextItNet(args1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXK5hBSJ2-SD"
      },
      "source": [
        "#### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ex9R-GF2-SD",
        "outputId": "ebdf26ee-a10d-435b-9a92-0c48a0a79a75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "++++++++++++++++++++ Train Epoch 1 ++++++++++++++++++++\n",
            "Training CE Loss: 11.35690\n",
            "one epoch train: 47.72131276130676\n",
            "++++++++++++++++++++ Valid Epoch 1 ++++++++++++++++++++\n",
            "{'Recall@20': 0.038265307034764974, 'NDCG@20': tensor(0.0193, device='cuda:0'), 'Recall@10': 0.038265307034764974, 'NDCG@10': tensor(0.0193, device='cuda:0'), 'Recall@5': 0.033801021320479255, 'NDCG@5': tensor(0.0180, device='cuda:0'), 'Recall@1': 0.004464285714285714, 'NDCG@1': tensor(0.0045, device='cuda:0')}\n",
            "one epoch val: 3.743847131729126\n",
            "++++++++++++++++++++ Train Epoch 2 ++++++++++++++++++++\n",
            "Training CE Loss: 10.42217\n",
            "one epoch train: 45.45848488807678\n",
            "++++++++++++++++++++ Valid Epoch 2 ++++++++++++++++++++\n",
            "{'Recall@20': 0.042729592749050686, 'NDCG@20': tensor(0.0188, device='cuda:0'), 'Recall@10': 0.033801021320479255, 'NDCG@10': tensor(0.0165, device='cuda:0'), 'Recall@5': 0.033801021320479255, 'NDCG@5': tensor(0.0165, device='cuda:0'), 'Recall@1': 0.004464285714285714, 'NDCG@1': tensor(0.0045, device='cuda:0')}\n",
            "one epoch val: 3.271419048309326\n",
            "++++++++++++++++++++ Train Epoch 3 ++++++++++++++++++++\n",
            "Training CE Loss: 10.21768\n",
            "one epoch train: 44.362900257110596\n",
            "++++++++++++++++++++ Valid Epoch 3 ++++++++++++++++++++\n",
            "{'Recall@20': 0.042729592749050686, 'NDCG@20': tensor(0.0218, device='cuda:0'), 'Recall@10': 0.038265307034764974, 'NDCG@10': tensor(0.0208, device='cuda:0'), 'Recall@5': 0.033801021320479255, 'NDCG@5': tensor(0.0194, device='cuda:0'), 'Recall@1': 0.004464285714285714, 'NDCG@1': tensor(0.0045, device='cuda:0')}\n",
            "one epoch val: 3.269226551055908\n",
            "++++++++++++++++++++ Train Epoch 4 ++++++++++++++++++++\n",
            "Training CE Loss: 10.04644\n",
            "one epoch train: 44.349186420440674\n",
            "++++++++++++++++++++ Valid Epoch 4 ++++++++++++++++++++\n",
            "{'Recall@20': 0.038265307034764974, 'NDCG@20': tensor(0.0208, device='cuda:0'), 'Recall@10': 0.038265307034764974, 'NDCG@10': tensor(0.0208, device='cuda:0'), 'Recall@5': 0.033801021320479255, 'NDCG@5': tensor(0.0194, device='cuda:0'), 'Recall@1': 0.004464285714285714, 'NDCG@1': tensor(0.0045, device='cuda:0')}\n",
            "one epoch val: 3.151986598968506\n",
            "++++++++++++++++++++ Train Epoch 5 ++++++++++++++++++++\n",
            "Training CE Loss: 9.86997\n",
            "one epoch train: 45.71024012565613\n",
            "++++++++++++++++++++ Valid Epoch 5 ++++++++++++++++++++\n",
            "{'Recall@20': 0.0471938784633364, 'NDCG@20': tensor(0.0275, device='cuda:0'), 'Recall@10': 0.038265307034764974, 'NDCG@10': tensor(0.0254, device='cuda:0'), 'Recall@5': 0.033801021320479255, 'NDCG@5': tensor(0.0240, device='cuda:0'), 'Recall@1': 0.008928571428571428, 'NDCG@1': tensor(0.0089, device='cuda:0')}\n",
            "one epoch val: 3.1512537002563477\n",
            "++++++++++++++++++++ Train Epoch 6 ++++++++++++++++++++\n",
            "Training CE Loss: 9.68498\n",
            "one epoch train: 47.25685429573059\n",
            "++++++++++++++++++++ Valid Epoch 6 ++++++++++++++++++++\n",
            "{'Recall@20': 0.042729592749050686, 'NDCG@20': tensor(0.0223, device='cuda:0'), 'Recall@10': 0.038265307034764974, 'NDCG@10': tensor(0.0212, device='cuda:0'), 'Recall@5': 0.033801021320479255, 'NDCG@5': tensor(0.0197, device='cuda:0'), 'Recall@1': 0.004464285714285714, 'NDCG@1': tensor(0.0045, device='cuda:0')}\n",
            "one epoch val: 3.305048704147339\n",
            "++++++++++++++++++++ Train Epoch 7 ++++++++++++++++++++\n",
            "Training CE Loss: 9.49035\n",
            "one epoch train: 46.10618782043457\n",
            "++++++++++++++++++++ Valid Epoch 7 ++++++++++++++++++++\n",
            "{'Recall@20': 0.0471938784633364, 'NDCG@20': tensor(0.0241, device='cuda:0'), 'Recall@10': 0.038265307034764974, 'NDCG@10': tensor(0.0219, device='cuda:0'), 'Recall@5': 0.033801021320479255, 'NDCG@5': tensor(0.0205, device='cuda:0'), 'Recall@1': 0.008928571428571428, 'NDCG@1': tensor(0.0089, device='cuda:0')}\n",
            "one epoch val: 2.8638999462127686\n",
            "++++++++++++++++++++ Train Epoch 8 ++++++++++++++++++++\n",
            "Training CE Loss: 9.28513\n",
            "one epoch train: 45.78419017791748\n",
            "++++++++++++++++++++ Valid Epoch 8 ++++++++++++++++++++\n",
            "{'Recall@20': 0.042729592749050686, 'NDCG@20': tensor(0.0248, device='cuda:0'), 'Recall@10': 0.038265307034764974, 'NDCG@10': tensor(0.0237, device='cuda:0'), 'Recall@5': 0.033801021320479255, 'NDCG@5': tensor(0.0224, device='cuda:0'), 'Recall@1': 0.004464285714285714, 'NDCG@1': tensor(0.0045, device='cuda:0')}\n",
            "one epoch val: 2.8893442153930664\n",
            "++++++++++++++++++++ Train Epoch 9 ++++++++++++++++++++\n",
            "Training CE Loss: 9.06553\n",
            "one epoch train: 45.73191547393799\n",
            "++++++++++++++++++++ Valid Epoch 9 ++++++++++++++++++++\n",
            "{'Recall@20': 0.042729592749050686, 'NDCG@20': tensor(0.0340, device='cuda:0'), 'Recall@10': 0.038265307034764974, 'NDCG@10': tensor(0.0329, device='cuda:0'), 'Recall@5': 0.033801021320479255, 'NDCG@5': tensor(0.0316, device='cuda:0'), 'Recall@1': 0.029336735606193542, 'NDCG@1': tensor(0.0293, device='cuda:0')}\n",
            "one epoch val: 2.8883020877838135\n",
            "++++++++++++++++++++ Train Epoch 10 ++++++++++++++++++++\n",
            "Training CE Loss: 8.82669\n",
            "one epoch train: 45.763843059539795\n",
            "++++++++++++++++++++ Valid Epoch 10 ++++++++++++++++++++\n",
            "{'Recall@20': 0.042729592749050686, 'NDCG@20': tensor(0.0192, device='cuda:0'), 'Recall@10': 0.038265307034764974, 'NDCG@10': tensor(0.0181, device='cuda:0'), 'Recall@5': 0.013392857142857142, 'NDCG@5': tensor(0.0095, device='cuda:0'), 'Recall@1': 0.004464285714285714, 'NDCG@1': tensor(0.0045, device='cuda:0')}\n",
            "one epoch val: 2.8935186862945557\n",
            "++++++++++++++++++++ Train Epoch 11 ++++++++++++++++++++\n",
            "Training CE Loss: 8.56395\n",
            "one epoch train: 45.78095626831055\n",
            "++++++++++++++++++++ Valid Epoch 11 ++++++++++++++++++++\n",
            "{'Recall@20': 0.042729592749050686, 'NDCG@20': tensor(0.0244, device='cuda:0'), 'Recall@10': 0.033801021320479255, 'NDCG@10': tensor(0.0219, device='cuda:0'), 'Recall@5': 0.033801021320479255, 'NDCG@5': tensor(0.0219, device='cuda:0'), 'Recall@1': 0.008928571428571428, 'NDCG@1': tensor(0.0089, device='cuda:0')}\n",
            "one epoch val: 2.8621909618377686\n",
            "++++++++++++++++++++ Train Epoch 12 ++++++++++++++++++++\n",
            "Training CE Loss: 8.28142\n",
            "one epoch train: 45.844452142715454\n",
            "++++++++++++++++++++ Valid Epoch 12 ++++++++++++++++++++\n",
            "{'Recall@20': 0.038265307034764974, 'NDCG@20': tensor(0.0196, device='cuda:0'), 'Recall@10': 0.038265307034764974, 'NDCG@10': tensor(0.0196, device='cuda:0'), 'Recall@5': 0.033801021320479255, 'NDCG@5': tensor(0.0183, device='cuda:0'), 'Recall@1': 0.004464285714285714, 'NDCG@1': tensor(0.0045, device='cuda:0')}\n",
            "one epoch val: 2.8924880027770996\n",
            "++++++++++++++++++++ Train Epoch 13 ++++++++++++++++++++\n",
            "Training CE Loss: 7.99065\n",
            "one epoch train: 45.74128437042236\n",
            "++++++++++++++++++++ Valid Epoch 13 ++++++++++++++++++++\n",
            "{'Recall@20': 0.042729592749050686, 'NDCG@20': tensor(0.0214, device='cuda:0'), 'Recall@10': 0.038265307034764974, 'NDCG@10': tensor(0.0202, device='cuda:0'), 'Recall@5': 0.033801021320479255, 'NDCG@5': tensor(0.0189, device='cuda:0'), 'Recall@1': 0.004464285714285714, 'NDCG@1': tensor(0.0045, device='cuda:0')}\n",
            "one epoch val: 2.8915345668792725\n",
            "++++++++++++++++++++ Train Epoch 14 ++++++++++++++++++++\n",
            "Training CE Loss: 7.69061\n",
            "one epoch train: 45.529558181762695\n",
            "++++++++++++++++++++ Valid Epoch 14 ++++++++++++++++++++\n",
            "{'Recall@20': 0.042729592749050686, 'NDCG@20': tensor(0.0246, device='cuda:0'), 'Recall@10': 0.033801021320479255, 'NDCG@10': tensor(0.0222, device='cuda:0'), 'Recall@5': 0.033801021320479255, 'NDCG@5': tensor(0.0222, device='cuda:0'), 'Recall@1': 0.013392857142857142, 'NDCG@1': tensor(0.0134, device='cuda:0')}\n",
            "one epoch val: 2.9416165351867676\n",
            "++++++++++++++++++++ Train Epoch 15 ++++++++++++++++++++\n",
            "Training CE Loss: 7.38120\n",
            "one epoch train: 45.713414907455444\n",
            "++++++++++++++++++++ Valid Epoch 15 ++++++++++++++++++++\n",
            "{'Recall@20': 0.042729592749050686, 'NDCG@20': tensor(0.0224, device='cuda:0'), 'Recall@10': 0.038265307034764974, 'NDCG@10': tensor(0.0211, device='cuda:0'), 'Recall@5': 0.033801021320479255, 'NDCG@5': tensor(0.0196, device='cuda:0'), 'Recall@1': 0.008928571428571428, 'NDCG@1': tensor(0.0089, device='cuda:0')}\n",
            "one epoch val: 2.817911148071289\n",
            "++++++++++++++++++++ Train Epoch 16 ++++++++++++++++++++\n",
            "Training CE Loss: 7.06309\n",
            "one epoch train: 46.64981484413147\n",
            "++++++++++++++++++++ Valid Epoch 16 ++++++++++++++++++++\n",
            "{'Recall@20': 0.042729592749050686, 'NDCG@20': tensor(0.0246, device='cuda:0'), 'Recall@10': 0.038265307034764974, 'NDCG@10': tensor(0.0234, device='cuda:0'), 'Recall@5': 0.033801021320479255, 'NDCG@5': tensor(0.0219, device='cuda:0'), 'Recall@1': 0.008928571428571428, 'NDCG@1': tensor(0.0089, device='cuda:0')}\n",
            "one epoch val: 3.053558111190796\n",
            "++++++++++++++++++++ Train Epoch 17 ++++++++++++++++++++\n",
            "Training CE Loss: 6.74334\n",
            "one epoch train: 48.545642375946045\n",
            "++++++++++++++++++++ Valid Epoch 17 ++++++++++++++++++++\n",
            "{'Recall@20': 0.042729592749050686, 'NDCG@20': tensor(0.0224, device='cuda:0'), 'Recall@10': 0.042729592749050686, 'NDCG@10': tensor(0.0224, device='cuda:0'), 'Recall@5': 0.033801021320479255, 'NDCG@5': tensor(0.0197, device='cuda:0'), 'Recall@1': 0.004464285714285714, 'NDCG@1': tensor(0.0045, device='cuda:0')}\n",
            "one epoch val: 2.9889118671417236\n",
            "++++++++++++++++++++ Train Epoch 18 ++++++++++++++++++++\n",
            "Training CE Loss: 6.42694\n",
            "one epoch train: 48.61962080001831\n",
            "++++++++++++++++++++ Valid Epoch 18 ++++++++++++++++++++\n",
            "{'Recall@20': 0.042729592749050686, 'NDCG@20': tensor(0.0225, device='cuda:0'), 'Recall@10': 0.038265307034764974, 'NDCG@10': tensor(0.0214, device='cuda:0'), 'Recall@5': 0.033801021320479255, 'NDCG@5': tensor(0.0200, device='cuda:0'), 'Recall@1': 0.008928571428571428, 'NDCG@1': tensor(0.0089, device='cuda:0')}\n",
            "one epoch val: 2.8036482334136963\n",
            "++++++++++++++++++++ Train Epoch 19 ++++++++++++++++++++\n",
            "Training CE Loss: 6.12723\n",
            "one epoch train: 46.062570095062256\n",
            "++++++++++++++++++++ Valid Epoch 19 ++++++++++++++++++++\n",
            "{'Recall@20': 0.042729592749050686, 'NDCG@20': tensor(0.0216, device='cuda:0'), 'Recall@10': 0.038265307034764974, 'NDCG@10': tensor(0.0204, device='cuda:0'), 'Recall@5': 0.033801021320479255, 'NDCG@5': tensor(0.0189, device='cuda:0'), 'Recall@1': 0.004464285714285714, 'NDCG@1': tensor(0.0045, device='cuda:0')}\n",
            "one epoch val: 2.8559906482696533\n",
            "++++++++++++++++++++ Train Epoch 20 ++++++++++++++++++++\n",
            "Training CE Loss: 5.85130\n",
            "one epoch train: 46.50890302658081\n",
            "++++++++++++++++++++ Valid Epoch 20 ++++++++++++++++++++\n",
            "{'Recall@20': 0.0471938784633364, 'NDCG@20': tensor(0.0238, device='cuda:0'), 'Recall@10': 0.038265307034764974, 'NDCG@10': tensor(0.0217, device='cuda:0'), 'Recall@5': 0.038265307034764974, 'NDCG@5': tensor(0.0217, device='cuda:0'), 'Recall@1': 0.004464285714285714, 'NDCG@1': tensor(0.0045, device='cuda:0')}\n",
            "one epoch val: 2.8262600898742676\n",
            "train_time: 923.2413322925568\n",
            "val_time: 60.36195683479309\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "NextItNet(\n",
              "  (item_embedding): Embedding(201149, 128, padding_idx=0)\n",
              "  (residual_blocks): Sequential(\n",
              "    (0): ResidualBlock_b(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1))\n",
              "      (ln1): LayerNorm((128,), eps=1e-08, elementwise_affine=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), dilation=(2, 2))\n",
              "      (ln2): LayerNorm((128,), eps=1e-08, elementwise_affine=True)\n",
              "    )\n",
              "    (1): ResidualBlock_b(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), dilation=(4, 4))\n",
              "      (ln1): LayerNorm((128,), eps=1e-08, elementwise_affine=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), dilation=(8, 8))\n",
              "      (ln2): LayerNorm((128,), eps=1e-08, elementwise_affine=True)\n",
              "    )\n",
              "    (2): ResidualBlock_b(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1))\n",
              "      (ln1): LayerNorm((128,), eps=1e-08, elementwise_affine=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), dilation=(2, 2))\n",
              "      (ln2): LayerNorm((128,), eps=1e-08, elementwise_affine=True)\n",
              "    )\n",
              "    (3): ResidualBlock_b(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), dilation=(4, 4))\n",
              "      (ln1): LayerNorm((128,), eps=1e-08, elementwise_affine=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), dilation=(8, 8))\n",
              "      (ln2): LayerNorm((128,), eps=1e-08, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (final_layer): Linear(in_features=128, out_features=201149, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Perform training and validation using the SeqTrain function with the specified configuration and hyperparameters.\n",
        "trainer.SeqTrain(args1.epochs, model1, train_dataloader1, valid_dataloader1, writer, args1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqPByQG92-SD"
      },
      "source": [
        "#### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAk-p9wU2-SD",
        "outputId": "66f8f189-54cb-4adc-a783-35c38a5ae8a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "++++++++++++++++++++ Valid Epoch 1 ++++++++++++++++++++\n",
            "{'Recall@20': 0.027994791666666668, 'NDCG@20': tensor(0.0112, device='cuda:0'), 'Recall@10': 0.017377804487179488, 'NDCG@10': tensor(0.0085, device='cuda:0'), 'Recall@5': 0.009515224358974358, 'NDCG@5': tensor(0.0060, device='cuda:0'), 'Recall@1': 0.002453926282051282, 'NDCG@1': tensor(0.0025, device='cuda:0')}\n"
          ]
        }
      ],
      "source": [
        "# Load the best-performing model weights from the saved checkpoint file.\n",
        "\n",
        "# The 'best_weight' variable is used to load the model weights from a specified checkpoint file. The file path is constructed based on the provided configuration and hyperparameters (args) to ensure the correct checkpoint file is loaded.\n",
        "\n",
        "best_weight1 = torch.load(os.path.join(args1.save_path, f'{args1.model_name}_seed{args1.seed}_is_pretrain_{args1.is_pretrain}_best_model_lr{args1.lr}_wd{args1.weight_decay}_block{args1.block_num}_hd{args1.hidden_size}_emb{args1.embedding_size}.pth'))\n",
        "\n",
        "# Load the model's state dictionary with the loaded best weights.\n",
        "\n",
        "# The model's state dictionary is updated with the best weights, effectively loading the best-performing model configuration.\n",
        "\n",
        "model1.load_state_dict(best_weight1)\n",
        "\n",
        "# Move the model to the specified device (GPU or CPU).\n",
        "\n",
        "# The 'model' is moved to the device specified in args.device, allowing computations and predictions to be performed on the chosen device.\n",
        "\n",
        "model1 = model1.to(args1.device)\n",
        "\n",
        "# Perform full sequence validation on the test dataset.\n",
        "\n",
        "# The Sequence_full_Validate function is called to evaluate the model's performance on the test dataset (test_dataloader).\n",
        "# The function takes the provided model, performs sequence validation, and logs relevant evaluation metrics using the specified SummaryWriter (writer) for TensorBoard.\n",
        "\n",
        "metrics1 = trainer.Sequence_full_Validate(0, model1, test_dataloader1, writer, args1, test=True)\n",
        "\n",
        "# Close the SummaryWriter.\n",
        "\n",
        "# The SummaryWriter is closed to finish logging the training and evaluation metrics for TensorBoard visualization and monitoring.\n",
        "\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtlrSbjX2-SD"
      },
      "source": [
        "## Insights, Observations, and Conclusion:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment 1 Results:\n",
        "\n",
        "### Training Loss and Validation Metrics:\n",
        "\n",
        "- During the training process, the Cross-Entropy (CE) loss decreases with each epoch, which is a positive sign as it indicates that the model is effectively learning from the training data.\n",
        "\n",
        "- However, the evaluation metrics (recall and NDCG) on the validation set remain consistently low and show no improvement throughout the training process. This indicates that the model is not able to generalize well to unseen data, leading to slightly good recommendation performance.\n",
        "\n",
        "### Model Architecture:\n",
        "\n",
        "The model architecture consists of an item embedding layer, followed by multiple residual blocks. The model then has a final linear layer for output.\n",
        "\n",
        "### Evaluation Results:\n",
        "\n",
        "The evaluation results on the validation set indicate good recommendation performance compared to the leaderboard here: https://tenrec0.github.io/. The NDCG@20 metric are consistently near to the leaderboard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- We trained this model on different dataset of users group where age = 3.\n",
        "- Despite the model learning from the training data (as evidenced by decreasing training loss), it generalized slightly good, leading to good recommendation performance.\n",
        "- The suboptimal results could be attributed to overfitting, inappropriate hyperparameters, or data preprocessing issues.\n",
        "- Further experimentation, including hyperparameter tuning and regularization is necessary to improve the model's recommendation capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1Sfeb0ClSDg"
      },
      "source": [
        "# Experiment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BztvDpXxlSDg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sessions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Number of items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "218252"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculate the number of unique items\n",
        "num_items2 = age2['item_id'].nunique()\n",
        "\n",
        "# Display the number of unique items\n",
        "num_items2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Number of users"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "21427"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculate the number of unique users\n",
        "num_users2 = age2['user_id'].nunique()\n",
        "\n",
        "# Display the number of unique users\n",
        "num_users2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Session Representation\n",
        "- Items Label Encoding\n",
        "- Session Initialization\n",
        "- Session Length Filtering\n",
        "- Session Sorting - already sorted\n",
        "- Session Padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Label Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The reason for using label encoding is that the 'item_id' column contains gaps between these IDs.\n",
        "- Label encoding is a suitable choice in this case to represent the 'item_id' values in a compact and sequential manner.\n",
        "- This approach helps in making the data representation more realistic, and it is particularly useful when working with categorical or nominal data where there is no intrinsic ordinal relationship between the categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a label encoding mapping for the item ids\n",
        "# The label encoding will assign unique integers to each unique 'item_id', starting from 1\n",
        "itemIdMapping2 = {k:i+1 for i, k in enumerate(sorted(list(age2['item_id'].unique())))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a new column 'item_id_mapped' by mapping the original 'item_id' values to their label encoded equivalents.\n",
        "age2[\"item_id_mapped\"] = age2['item_id'].map(itemIdMapping2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>age</th>\n",
              "      <th>item_id_mapped</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>247</th>\n",
              "      <td>8</td>\n",
              "      <td>97</td>\n",
              "      <td>0</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248</th>\n",
              "      <td>8</td>\n",
              "      <td>98</td>\n",
              "      <td>0</td>\n",
              "      <td>87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249</th>\n",
              "      <td>8</td>\n",
              "      <td>99</td>\n",
              "      <td>0</td>\n",
              "      <td>88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250</th>\n",
              "      <td>8</td>\n",
              "      <td>106</td>\n",
              "      <td>0</td>\n",
              "      <td>94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>8</td>\n",
              "      <td>400846</td>\n",
              "      <td>0</td>\n",
              "      <td>134343</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     user_id  item_id  age  item_id_mapped\n",
              "247        8       97    0              86\n",
              "248        8       98    0              87\n",
              "249        8       99    0              88\n",
              "250        8      106    0              94\n",
              "251        8   400846    0          134343"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display the first 5 rows of the DataFrame 'age2'\n",
        "age2.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Session Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- This operation effectively organizes the data into sessions based on user interactions.\n",
        "- Each session represents a sequence of items (represented by their label encoded integers) that a user has interacted with, and these sequences are grouped according to each user's session.\n",
        "- The 'sessions' DataFrame will be useful for building and training session-based recommendation models that leverage the sequential patterns of user-item interactions for personalized recommendations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group the DataFrame 'age2' by 'user_id' and aggregate the corresponding 'item_id_mapped' values into lists.\n",
        "# Each session corresponds to a user, and the 'item_id_mapped' values for that user are collected in a list.\n",
        "# The resulting DataFrame 'sessions' will have each user_id as the index and a list of corresponding 'item_id_mapped' values as the 'item_id_mapped' column.\n",
        "\n",
        "sessions2 = age2.groupby(\"user_id\")[['item_id_mapped']].agg(list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item_id_mapped</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>user_id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[86, 87, 88, 94, 134343, 16722, 124432, 10473,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[101, 102, 103, 104, 105, 106, 107, 108, 109, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>[328, 331, 218, 336, 338, 36, 9219, 5219, 2995...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>[457, 458, 460, 461, 462, 88778, 74620, 60811,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>[71, 992, 38151, 27710, 29007, 3336, 24492, 43...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            item_id_mapped\n",
              "user_id                                                   \n",
              "8        [86, 87, 88, 94, 134343, 16722, 124432, 10473,...\n",
              "10       [101, 102, 103, 104, 105, 106, 107, 108, 109, ...\n",
              "31       [328, 331, 218, 336, 338, 36, 9219, 5219, 2995...\n",
              "43       [457, 458, 460, 461, 462, 88778, 74620, 60811,...\n",
              "53       [71, 992, 38151, 27710, 29007, 3336, 24492, 43..."
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display the first 5 rows of the DataFrame 'sessions'\n",
        "sessions2.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Session Length Filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the minimum and maximum sequence lengths for session-based recommendation based on the paper.\n",
        "\n",
        "# Minimum Sequence Length:\n",
        "# The 'min_sequence_length' variable is set to 10, which specifies the minimum number of items required in a session (user's interaction sequence).\n",
        "# Sessions with fewer than 10 items may be excluded from the analysis to ensure that sessions used for training have a minimum length to capture meaningful patterns.\n",
        "\n",
        "min_sequence_length = 10\n",
        "\n",
        "# Maximum Sequence Length:\n",
        "# The 'max_sequence_length' variable is set to 30, which represents the maximum allowed number of items in a session (user's interaction sequence).\n",
        "# Sessions with more than 30 items may be truncated to this maximum length to avoid excessively long sequences, ensuring computational efficiency during training.\n",
        "\n",
        "max_sequence_length = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter the 'sessions' DataFrame to include only sessions with a length greater than the 10.\n",
        "sessions2 = sessions2[sessions2['item_id_mapped'].apply(lambda x: len(x) > min_sequence_length)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter the 'sessions' DataFrame to include only sessions with a length less than the 30.\n",
        "sessions2['item_id_mapped'] = sessions2['item_id_mapped'].apply(lambda x: x[:max_sequence_length])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([list([86, 87, 88, 94, 134343, 16722, 124432, 10473, 2233, 5580, 1765, 3864, 26341, 509, 12567, 10385, 28147, 41822, 17046, 5917, 16777, 140804, 62885, 14855, 1564, 27380, 4776, 98, 1324, 20751]),\n",
              "       list([101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 113, 115, 133738, 133739, 133740, 147035, 147036, 147037, 12308, 152885, 152886, 156082, 5896, 158074, 158075, 158076, 5135, 112621, 124310, 167891]),\n",
              "       list([328, 331, 218, 336, 338, 36, 9219, 5219, 2995, 7662, 10987, 3042, 27121, 6899, 24541, 1817, 5842, 249, 2194, 36037, 640, 16002, 2532, 1608, 3027]),\n",
              "       ...,\n",
              "       list([85515, 45722, 14713, 37003, 91335, 19563, 895, 19138, 34347, 56020, 130018, 28565, 106004, 19126, 17925, 114161, 5089, 29644, 8654, 122976, 22156, 28774, 116818, 7883, 182, 18255, 21486, 21647, 130015, 27122]),\n",
              "       list([17144, 475, 27110, 2288, 151097, 3823, 30483, 1761, 5925, 44515, 1650, 1779, 124643, 17016, 6278, 2628, 64466, 76092, 3407, 34782, 8860]),\n",
              "       list([16899, 86174, 3539, 21894, 4964, 2967, 38710, 61, 31139, 93992, 10716, 664, 127, 597, 14094, 80552, 226, 6727, 2450, 4470, 28118, 6111, 1246, 8514, 12764, 61405, 6355, 108911, 29580, 12152])],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display the 'item_id_mapped' values.\n",
        "sessions2['item_id_mapped'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Session Padding\n",
        "add padding to the beginning of each sequance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pad the 'item_id_mapped' lists with zeros to make each sequence length equal to the defined maximum sequence length.\n",
        "sessions2['item_id_mapped'] = sessions2['item_id_mapped'].apply(lambda x: [0] * (max_sequence_length - len(x)) + x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item_id_mapped</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>user_id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[86, 87, 88, 94, 134343, 16722, 124432, 10473,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[101, 102, 103, 104, 105, 106, 107, 108, 109, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>[0, 0, 0, 0, 0, 328, 331, 218, 336, 338, 36, 9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>[457, 458, 460, 461, 462, 88778, 74620, 60811,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>[71, 992, 38151, 27710, 29007, 3336, 24492, 43...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            item_id_mapped\n",
              "user_id                                                   \n",
              "8        [86, 87, 88, 94, 134343, 16722, 124432, 10473,...\n",
              "10       [101, 102, 103, 104, 105, 106, 107, 108, 109, ...\n",
              "31       [0, 0, 0, 0, 0, 328, 331, 218, 336, 338, 36, 9...\n",
              "43       [457, 458, 460, 461, 462, 88778, 74620, 60811,...\n",
              "53       [71, 992, 38151, 27710, 29007, 3336, 24492, 43..."
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display the first 5 rows of the DataFrame 'sessions'\n",
        "sessions2.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Common Argument"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the 'args' Namespace to store configuration and hyperparameters for the session-based recommendation model training.\n",
        "\n",
        "args2 = argparse.Namespace(\n",
        "    seed=seed,                             # Seed for reproducibility\n",
        "    save_path='./checkpoint/',             # Directory to save model checkpoints\n",
        "    model_name='NextItNet2',               # Name of the best model checkpoint\n",
        "\n",
        "    device='cuda',                         # Device to use for training ('cuda' for GPU, 'cpu' for CPU)\n",
        "\n",
        "    is_parallel=False,                     # Flag to enable/disable model parallelism (multi-GPU training)\n",
        "\n",
        "    # get_data arguments\n",
        "    item_min=min_sequence_length,          # Minimum sequence length for a session\n",
        "    valid_rate=100,                        # Ratio of data samples to use for validation\n",
        "    max_len=max_sequence_length,           # Maximum allowed sequence length for a session\n",
        "    bert_mask_prob=0.3,                    # Probability for applying BERT-style masking to sequence data\n",
        "    pad_token=0,                           # Token value used for padding sequences\n",
        "    train_batch_size=32,                   # Batch size for training data\n",
        "    test_batch_size=32,                    # Batch size for testing data\n",
        "    val_batch_size=32,                     # Batch size for validation data\n",
        "\n",
        "    # load_model arguments\n",
        "    embedding_size=128,                   # Size of the item embedding vector\n",
        "    block_num=2,                          # Number of blocks in the model\n",
        "    dilations=[1, 4],                     # Dilation rates for the blocks\n",
        "    kernel_size=3,                        # Size of the convolutional kernel\n",
        "    num_items=num_items2,                  # Number of unique items in the dataset\n",
        "    num_users=num_users2,                  # Number of unique users in the dataset\n",
        "    num_heads=4,                          # Number of attention heads in the transformer\n",
        "    dropout=0.1,                          # Dropout rate for regularization\n",
        "\n",
        "    # SeqTrain arguments\n",
        "    epochs=20,                            # Number of training epochs\n",
        "    is_pretrain=1,                        # Flag for pretraining mode (0: pretrain, 1: train from scratch)\n",
        "    lr=0.0001,                            # Learning rate for optimization\n",
        "    weight_decay=0.0,                     # Weight decay regularization parameter\n",
        "    local_rank=None,                      # Local rank for distributed training\n",
        "    metric_ks=[1, 5, 10, 20],             # Values of 'k' for evaluating top-k metrics (e.g., top-1, top-5, etc.)\n",
        "    hidden_size=16,                       # Size of the hidden layer in the NextItNet model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the device\n",
        "device = torch.device(args2.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C5YjLAflSDj"
      },
      "source": [
        "## Train-Test-Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TOhsIHhlSDj",
        "outputId": "3bb9d31a-9747-406c-a5ed-03c55c82e202"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/20631 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20631/20631 [00:00<00:00, 109879.99it/s]\n"
          ]
        }
      ],
      "source": [
        "# Split the session data into training, validation, and test sets using train_val_test_split function.\n",
        "train_data2, val_data2, test_data2 = dataset.train_val_test_split(sessions2['item_id_mapped'].apply(list).to_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "V9dckRaTlSDk"
      },
      "outputs": [],
      "source": [
        "# Split the train_data and val_data dictionaries into smaller subsets for session-based recommendation model validation.\n",
        "\n",
        "# Initialize empty dictionaries to store the smaller train and validation data subsets.\n",
        "train_data_s2, val_data_s2 = {}, {}\n",
        "\n",
        "# Get the total number of data samples in the train_data dictionary.\n",
        "data_len2 = len(train_data2)\n",
        "\n",
        "# Set up a loop to iterate over the keys (sessions) in the val_data dictionary.\n",
        "# The loop copies the session data from the original train_data and val_data dictionaries into the smaller subsets train_data_s and val_data_s.\n",
        "# The loop stops when a fraction of the data (specified by args.valid_rate) has been copied into the validation subset.\n",
        "\n",
        "i = 0\n",
        "for key, _ in val_data2.items():\n",
        "    # Copy the session data from train_data into train_data_s.\n",
        "    train_data_s2[key] = train_data2[key]\n",
        "\n",
        "    # Copy the session data from val_data into val_data_s.\n",
        "    val_data_s2[key] = val_data2[key]\n",
        "\n",
        "    # Increment the counter 'i' to track the number of iterations.\n",
        "    i += 1\n",
        "\n",
        "    # Check if the number of iterations has reached the fraction of the total data specified by args.valid_rate.\n",
        "    # If it has, stop the loop to limit the size of the validation subset.\n",
        "    if i == int(data_len2 / args2.valid_rate):\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ke8cgj7PlSDk"
      },
      "outputs": [],
      "source": [
        "# Create three datasets for session-based recommendation model training and evaluation.\n",
        "\n",
        "# 1. train_dataset:\n",
        "# The train_dataset is built using the BuildTrainDataset class, which constructs a training dataset from the train_data dictionary.\n",
        "# The training dataset includes input sequences (sessions) from train_data with a maximum length of args.max_len, padded with args.pad_token to reach the maximum length.\n",
        "# args.num_items represents the number of unique items in the dataset, which is used for encoding the item IDs.\n",
        "\n",
        "train_dataset2 = dataset.BuildTrainDataset(train_data2, args2.max_len, args2.pad_token, args2.num_items)\n",
        "\n",
        "# 2. valid_dataset:\n",
        "# The valid_dataset is constructed using the Build_full_EvalDataset class, which builds a full evaluation dataset from the train_data_s and val_data_s dictionaries.\n",
        "# The full evaluation dataset combines training and validation data, allowing evaluation on both datasets during model training.\n",
        "# The input sequences (sessions) from both train_data_s and val_data_s have a maximum length of args.max_len and are padded with args.pad_token to reach the maximum length.\n",
        "# args.num_items represents the number of unique items in the dataset, which is used for encoding the item IDs.\n",
        "\n",
        "valid_dataset2 = dataset.Build_full_EvalDataset(train_data_s2, val_data_s2, args2.max_len, args2.pad_token, args2.num_items)\n",
        "\n",
        "# 3. test_dataset:\n",
        "# The test_dataset is created using the Build_full_EvalDataset class, which builds a full evaluation dataset from the train_data and test_data dictionaries.\n",
        "# Similar to the valid_dataset, the full evaluation dataset combines training and testing data to allow evaluation on both datasets after model training.\n",
        "# The input sequences (sessions) from both train_data and test_data have a maximum length of args.max_len and are padded with args.pad_token to reach the maximum length.\n",
        "# args.num_items represents the number of unique items in the dataset, which is used for encoding the item IDs.\n",
        "\n",
        "test_dataset2 = dataset.Build_full_EvalDataset(train_data2, test_data2, args2.max_len, args2.pad_token, args2.num_items)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "VkhExZghlSDk"
      },
      "outputs": [],
      "source": [
        "# Create three data loaders for the session-based recommendation model training and evaluation.\n",
        "\n",
        "# 1. train_dataloader:\n",
        "# The train_dataloader is created using the get_train_loader function, which generates a data loader for the train_dataset.\n",
        "# The train_dataset contains training samples that are organized in batches to facilitate efficient model training.\n",
        "# The get_train_loader function handles batch creation, taking into account the training batch size specified in args.train_batch_size.\n",
        "\n",
        "train_dataloader2 = dataset.get_train_loader(train_dataset2, args2)\n",
        "\n",
        "# 2. valid_dataloader:\n",
        "# The valid_dataloader is generated using the get_val_loader function, which creates a data loader for the valid_dataset.\n",
        "# The valid_dataset includes a combination of training and validation data, used for model validation during training.\n",
        "# Similar to the train_dataloader, the get_val_loader function manages batch creation, considering the validation batch size specified in args.val_batch_size.\n",
        "\n",
        "valid_dataloader2 = dataset.get_val_loader(valid_dataset2, args2)\n",
        "\n",
        "# 3. test_dataloader:\n",
        "# The test_dataloader is constructed using the get_test_loader function, which generates a data loader for the test_dataset.\n",
        "# The test_dataset combines training and testing data, facilitating model evaluation on both datasets after training.\n",
        "# Like the previous dataloaders, the get_test_loader function handles batch creation, considering the test batch size specified in args.test_batch_size.\n",
        "\n",
        "test_dataloader2 = dataset.get_test_loader(test_dataset2, args2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Selection\n",
        "\n",
        "Steps for Session-Based Recommendation:\n",
        "\n",
        "- Research and Understanding: Familiarize ourself with the characteristics, strengths, and limitations of each model considered for session-based recommendation, namely BERT4Rec, GRU4Rec, SASRec, and NextItNet. Gain insights into their architectures, training approaches.\n",
        "\n",
        "- Evaluation Metrics: Determine the evaluation metrics that align with your project goals, such as recall and Normalized Discounted Cumulative Gain (NDCG). Ensure that the selected models can be evaluated using these metrics.\n",
        "\n",
        "- Implementation Suitability: Assess the feasibility and compatibility of each model with the dataset, session-based recommendation task, and available computational resources.\n",
        "\n",
        "- Robustness and Generalization: Analyze the robustness and generalization capabilities of each model by assessing their performance across different subsets for example different ages groups as we did. Consider their ability to handle various session lengths we tried different values of session min/max sequence length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-rpPb6llSDk"
      },
      "source": [
        "### NextItNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-19nvwnxlSDk"
      },
      "source": [
        "### Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "arQ0LgxQlSDk"
      },
      "outputs": [],
      "source": [
        "# Instantiate the NextItNet model with the provided configuration and hyperparameters.\n",
        "model2 = NextItNet(args2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPBBd-vblSDk"
      },
      "source": [
        "#### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyVeACGelSDk",
        "outputId": "5707487d-d16f-4787-d180-ed493ef68df5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "++++++++++++++++++++ Train Epoch 1 ++++++++++++++++++++\n",
            "Training CE Loss: 11.63412\n",
            "one epoch train: 54.2485294342041\n",
            "++++++++++++++++++++ Valid Epoch 1 ++++++++++++++++++++\n",
            "{'Recall@20': 0.013392857142857142, 'NDCG@20': tensor(0.0072, device='cuda:0'), 'Recall@10': 0.013392857142857142, 'NDCG@10': tensor(0.0072, device='cuda:0'), 'Recall@5': 0.004464285714285714, 'NDCG@5': tensor(0.0045, device='cuda:0'), 'Recall@1': 0.004464285714285714, 'NDCG@1': tensor(0.0045, device='cuda:0')}\n",
            "one epoch val: 3.320791482925415\n",
            "++++++++++++++++++++ Train Epoch 2 ++++++++++++++++++++\n",
            "Training CE Loss: 10.71244\n",
            "one epoch train: 50.592814683914185\n",
            "++++++++++++++++++++ Valid Epoch 2 ++++++++++++++++++++\n",
            "{'Recall@20': 0.013392857142857142, 'NDCG@20': tensor(0.0070, device='cuda:0'), 'Recall@10': 0.008928571428571428, 'NDCG@10': tensor(0.0058, device='cuda:0'), 'Recall@5': 0.004464285714285714, 'NDCG@5': tensor(0.0045, device='cuda:0'), 'Recall@1': 0.004464285714285714, 'NDCG@1': tensor(0.0045, device='cuda:0')}\n",
            "one epoch val: 3.2206554412841797\n",
            "++++++++++++++++++++ Train Epoch 3 ++++++++++++++++++++\n",
            "Training CE Loss: 10.49045\n",
            "one epoch train: 50.592018604278564\n",
            "++++++++++++++++++++ Valid Epoch 3 ++++++++++++++++++++\n",
            "{'Recall@20': 0.013392857142857142, 'NDCG@20': tensor(0.0069, device='cuda:0'), 'Recall@10': 0.004464285714285714, 'NDCG@10': tensor(0.0045, device='cuda:0'), 'Recall@5': 0.004464285714285714, 'NDCG@5': tensor(0.0045, device='cuda:0'), 'Recall@1': 0.004464285714285714, 'NDCG@1': tensor(0.0045, device='cuda:0')}\n",
            "one epoch val: 3.533374786376953\n",
            "++++++++++++++++++++ Train Epoch 4 ++++++++++++++++++++\n",
            "Training CE Loss: 10.30796\n",
            "one epoch train: 57.07728910446167\n",
            "++++++++++++++++++++ Valid Epoch 4 ++++++++++++++++++++\n",
            "{'Recall@20': 0.008928571428571428, 'NDCG@20': tensor(0.0057, device='cuda:0'), 'Recall@10': 0.004464285714285714, 'NDCG@10': tensor(0.0045, device='cuda:0'), 'Recall@5': 0.004464285714285714, 'NDCG@5': tensor(0.0045, device='cuda:0'), 'Recall@1': 0.004464285714285714, 'NDCG@1': tensor(0.0045, device='cuda:0')}\n",
            "one epoch val: 3.6493828296661377\n",
            "++++++++++++++++++++ Train Epoch 5 ++++++++++++++++++++\n",
            "Training CE Loss: 10.12554\n",
            "one epoch train: 59.019617319107056\n",
            "++++++++++++++++++++ Valid Epoch 5 ++++++++++++++++++++\n",
            "{'Recall@20': 0.013392857142857142, 'NDCG@20': tensor(0.0068, device='cuda:0'), 'Recall@10': 0.004464285714285714, 'NDCG@10': tensor(0.0045, device='cuda:0'), 'Recall@5': 0.004464285714285714, 'NDCG@5': tensor(0.0045, device='cuda:0'), 'Recall@1': 0.004464285714285714, 'NDCG@1': tensor(0.0045, device='cuda:0')}\n",
            "one epoch val: 3.530978202819824\n",
            "++++++++++++++++++++ Train Epoch 6 ++++++++++++++++++++\n",
            "Training CE Loss: 9.93148\n",
            "one epoch train: 71.21053218841553\n",
            "++++++++++++++++++++ Valid Epoch 6 ++++++++++++++++++++\n",
            "{'Recall@20': 0.013392857142857142, 'NDCG@20': tensor(0.0070, device='cuda:0'), 'Recall@10': 0.008928571428571428, 'NDCG@10': tensor(0.0058, device='cuda:0'), 'Recall@5': 0.004464285714285714, 'NDCG@5': tensor(0.0045, device='cuda:0'), 'Recall@1': 0.004464285714285714, 'NDCG@1': tensor(0.0045, device='cuda:0')}\n",
            "one epoch val: 3.582468271255493\n",
            "++++++++++++++++++++ Train Epoch 7 ++++++++++++++++++++\n",
            "Training CE Loss: 9.72529\n",
            "one epoch train: 96.91549754142761\n",
            "++++++++++++++++++++ Valid Epoch 7 ++++++++++++++++++++\n",
            "{'Recall@20': 0.013392857142857142, 'NDCG@20': tensor(0.0052, device='cuda:0'), 'Recall@10': 0.004464285714285714, 'NDCG@10': tensor(0.0028, device='cuda:0'), 'Recall@5': 0.004464285714285714, 'NDCG@5': tensor(0.0028, device='cuda:0'), 'Recall@1': 0.0, 'NDCG@1': tensor(0., device='cuda:0')}\n",
            "one epoch val: 3.4900970458984375\n",
            "++++++++++++++++++++ Train Epoch 8 ++++++++++++++++++++\n",
            "Training CE Loss: 9.50613\n",
            "one epoch train: 88.48512125015259\n",
            "++++++++++++++++++++ Valid Epoch 8 ++++++++++++++++++++\n",
            "{'Recall@20': 0.013392857142857142, 'NDCG@20': tensor(0.0068, device='cuda:0'), 'Recall@10': 0.004464285714285714, 'NDCG@10': tensor(0.0045, device='cuda:0'), 'Recall@5': 0.004464285714285714, 'NDCG@5': tensor(0.0045, device='cuda:0'), 'Recall@1': 0.004464285714285714, 'NDCG@1': tensor(0.0045, device='cuda:0')}\n",
            "one epoch val: 3.478634834289551\n",
            "++++++++++++++++++++ Train Epoch 9 ++++++++++++++++++++\n",
            "Training CE Loss: 9.27185\n",
            "one epoch train: 72.16090226173401\n",
            "++++++++++++++++++++ Valid Epoch 9 ++++++++++++++++++++\n",
            "{'Recall@20': 0.017857142857142856, 'NDCG@20': tensor(0.0081, device='cuda:0'), 'Recall@10': 0.008928571428571428, 'NDCG@10': tensor(0.0058, device='cuda:0'), 'Recall@5': 0.004464285714285714, 'NDCG@5': tensor(0.0045, device='cuda:0'), 'Recall@1': 0.004464285714285714, 'NDCG@1': tensor(0.0045, device='cuda:0')}\n",
            "one epoch val: 3.6450626850128174\n",
            "++++++++++++++++++++ Train Epoch 10 ++++++++++++++++++++\n",
            "Training CE Loss: 9.01497\n",
            "one epoch train: 66.13178133964539\n",
            "++++++++++++++++++++ Valid Epoch 10 ++++++++++++++++++++\n",
            "{'Recall@20': 0.022321428571428572, 'NDCG@20': tensor(0.0074, device='cuda:0'), 'Recall@10': 0.008928571428571428, 'NDCG@10': tensor(0.0041, device='cuda:0'), 'Recall@5': 0.004464285714285714, 'NDCG@5': tensor(0.0028, device='cuda:0'), 'Recall@1': 0.0, 'NDCG@1': tensor(0., device='cuda:0')}\n",
            "one epoch val: 3.592477560043335\n",
            "++++++++++++++++++++ Train Epoch 11 ++++++++++++++++++++\n",
            "Training CE Loss: 8.72760\n",
            "one epoch train: 65.54763650894165\n",
            "++++++++++++++++++++ Valid Epoch 11 ++++++++++++++++++++\n",
            "{'Recall@20': 0.026785714285714284, 'NDCG@20': tensor(0.0085, device='cuda:0'), 'Recall@10': 0.008928571428571428, 'NDCG@10': tensor(0.0041, device='cuda:0'), 'Recall@5': 0.004464285714285714, 'NDCG@5': tensor(0.0028, device='cuda:0'), 'Recall@1': 0.0, 'NDCG@1': tensor(0., device='cuda:0')}\n",
            "one epoch val: 3.577407121658325\n",
            "++++++++++++++++++++ Train Epoch 12 ++++++++++++++++++++\n",
            "Training CE Loss: 8.41538\n",
            "one epoch train: 65.81828451156616\n",
            "++++++++++++++++++++ Valid Epoch 12 ++++++++++++++++++++\n",
            "{'Recall@20': 0.026785714285714284, 'NDCG@20': tensor(0.0086, device='cuda:0'), 'Recall@10': 0.004464285714285714, 'NDCG@10': tensor(0.0028, device='cuda:0'), 'Recall@5': 0.004464285714285714, 'NDCG@5': tensor(0.0028, device='cuda:0'), 'Recall@1': 0.0, 'NDCG@1': tensor(0., device='cuda:0')}\n",
            "one epoch val: 3.6165194511413574\n",
            "++++++++++++++++++++ Train Epoch 13 ++++++++++++++++++++\n",
            "Training CE Loss: 8.09413\n",
            "one epoch train: 65.87055373191833\n",
            "++++++++++++++++++++ Valid Epoch 13 ++++++++++++++++++++\n",
            "{'Recall@20': 0.013392857142857142, 'NDCG@20': tensor(0.0056, device='cuda:0'), 'Recall@10': 0.013392857142857142, 'NDCG@10': tensor(0.0056, device='cuda:0'), 'Recall@5': 0.004464285714285714, 'NDCG@5': tensor(0.0028, device='cuda:0'), 'Recall@1': 0.0, 'NDCG@1': tensor(0., device='cuda:0')}\n",
            "one epoch val: 3.556886672973633\n",
            "++++++++++++++++++++ Train Epoch 14 ++++++++++++++++++++\n",
            "Training CE Loss: 7.76014\n",
            "one epoch train: 65.94125986099243\n",
            "++++++++++++++++++++ Valid Epoch 14 ++++++++++++++++++++\n",
            "{'Recall@20': 0.017857142857142856, 'NDCG@20': tensor(0.0083, device='cuda:0'), 'Recall@10': 0.008928571428571428, 'NDCG@10': tensor(0.0060, device='cuda:0'), 'Recall@5': 0.004464285714285714, 'NDCG@5': tensor(0.0045, device='cuda:0'), 'Recall@1': 0.004464285714285714, 'NDCG@1': tensor(0.0045, device='cuda:0')}\n",
            "one epoch val: 3.751619815826416\n",
            "++++++++++++++++++++ Train Epoch 15 ++++++++++++++++++++\n",
            "Training CE Loss: 7.41256\n",
            "one epoch train: 66.15538096427917\n",
            "++++++++++++++++++++ Valid Epoch 15 ++++++++++++++++++++\n",
            "{'Recall@20': 0.026785714285714284, 'NDCG@20': tensor(0.0083, device='cuda:0'), 'Recall@10': 0.004464285714285714, 'NDCG@10': tensor(0.0028, device='cuda:0'), 'Recall@5': 0.004464285714285714, 'NDCG@5': tensor(0.0028, device='cuda:0'), 'Recall@1': 0.0, 'NDCG@1': tensor(0., device='cuda:0')}\n",
            "one epoch val: 3.6386897563934326\n",
            "++++++++++++++++++++ Train Epoch 16 ++++++++++++++++++++\n",
            "Training CE Loss: 7.05446\n",
            "one epoch train: 65.84346628189087\n",
            "++++++++++++++++++++ Valid Epoch 16 ++++++++++++++++++++\n",
            "{'Recall@20': 0.026785714285714284, 'NDCG@20': tensor(0.0101, device='cuda:0'), 'Recall@10': 0.004464285714285714, 'NDCG@10': tensor(0.0045, device='cuda:0'), 'Recall@5': 0.004464285714285714, 'NDCG@5': tensor(0.0045, device='cuda:0'), 'Recall@1': 0.004464285714285714, 'NDCG@1': tensor(0.0045, device='cuda:0')}\n",
            "one epoch val: 3.650474786758423\n",
            "++++++++++++++++++++ Train Epoch 17 ++++++++++++++++++++\n",
            "Training CE Loss: 6.69518\n",
            "one epoch train: 65.49485874176025\n",
            "++++++++++++++++++++ Valid Epoch 17 ++++++++++++++++++++\n",
            "{'Recall@20': 0.022321428571428572, 'NDCG@20': tensor(0.0091, device='cuda:0'), 'Recall@10': 0.008928571428571428, 'NDCG@10': tensor(0.0058, device='cuda:0'), 'Recall@5': 0.004464285714285714, 'NDCG@5': tensor(0.0045, device='cuda:0'), 'Recall@1': 0.004464285714285714, 'NDCG@1': tensor(0.0045, device='cuda:0')}\n",
            "one epoch val: 3.7923359870910645\n",
            "++++++++++++++++++++ Train Epoch 18 ++++++++++++++++++++\n",
            "Training CE Loss: 6.34468\n",
            "one epoch train: 65.79289841651917\n",
            "++++++++++++++++++++ Valid Epoch 18 ++++++++++++++++++++\n",
            "{'Recall@20': 0.026785714285714284, 'NDCG@20': tensor(0.0103, device='cuda:0'), 'Recall@10': 0.008928571428571428, 'NDCG@10': tensor(0.0058, device='cuda:0'), 'Recall@5': 0.004464285714285714, 'NDCG@5': tensor(0.0045, device='cuda:0'), 'Recall@1': 0.004464285714285714, 'NDCG@1': tensor(0.0045, device='cuda:0')}\n",
            "one epoch val: 3.591768980026245\n",
            "++++++++++++++++++++ Train Epoch 19 ++++++++++++++++++++\n",
            "Training CE Loss: 6.01905\n",
            "one epoch train: 65.6593668460846\n",
            "++++++++++++++++++++ Valid Epoch 19 ++++++++++++++++++++\n",
            "{'Recall@20': 0.022321428571428572, 'NDCG@20': tensor(0.0077, device='cuda:0'), 'Recall@10': 0.008928571428571428, 'NDCG@10': tensor(0.0041, device='cuda:0'), 'Recall@5': 0.004464285714285714, 'NDCG@5': tensor(0.0028, device='cuda:0'), 'Recall@1': 0.0, 'NDCG@1': tensor(0., device='cuda:0')}\n",
            "one epoch val: 3.6651110649108887\n",
            "++++++++++++++++++++ Train Epoch 20 ++++++++++++++++++++\n",
            "Training CE Loss: 5.72193\n",
            "one epoch train: 65.87328457832336\n",
            "++++++++++++++++++++ Valid Epoch 20 ++++++++++++++++++++\n",
            "{'Recall@20': 0.022321428571428572, 'NDCG@20': tensor(0.0076, device='cuda:0'), 'Recall@10': 0.013392857142857142, 'NDCG@10': tensor(0.0054, device='cuda:0'), 'Recall@5': 0.004464285714285714, 'NDCG@5': tensor(0.0028, device='cuda:0'), 'Recall@1': 0.0, 'NDCG@1': tensor(0., device='cuda:0')}\n",
            "one epoch val: 3.71474552154541\n",
            "train_time: 1324.4310941696167\n",
            "val_time: 71.59948229789734\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "NextItNet(\n",
              "  (item_embedding): Embedding(218253, 128, padding_idx=0)\n",
              "  (residual_blocks): Sequential(\n",
              "    (0): ResidualBlock_b(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1))\n",
              "      (ln1): LayerNorm((128,), eps=1e-08, elementwise_affine=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), dilation=(2, 2))\n",
              "      (ln2): LayerNorm((128,), eps=1e-08, elementwise_affine=True)\n",
              "    )\n",
              "    (1): ResidualBlock_b(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), dilation=(4, 4))\n",
              "      (ln1): LayerNorm((128,), eps=1e-08, elementwise_affine=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), dilation=(8, 8))\n",
              "      (ln2): LayerNorm((128,), eps=1e-08, elementwise_affine=True)\n",
              "    )\n",
              "    (2): ResidualBlock_b(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1))\n",
              "      (ln1): LayerNorm((128,), eps=1e-08, elementwise_affine=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), dilation=(2, 2))\n",
              "      (ln2): LayerNorm((128,), eps=1e-08, elementwise_affine=True)\n",
              "    )\n",
              "    (3): ResidualBlock_b(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), dilation=(4, 4))\n",
              "      (ln1): LayerNorm((128,), eps=1e-08, elementwise_affine=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), dilation=(8, 8))\n",
              "      (ln2): LayerNorm((128,), eps=1e-08, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (final_layer): Linear(in_features=128, out_features=218253, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Perform training and validation using the SeqTrain function with the specified configuration and hyperparameters.\n",
        "trainer.SeqTrain(args2.epochs, model2, train_dataloader2, valid_dataloader2, writer, args2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peBbyTEilSDk"
      },
      "source": [
        "#### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlFRNWdYlSDk",
        "outputId": "1ff2cbe4-2a02-4111-965e-7961f8f54de7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "++++++++++++++++++++ Valid Epoch 1 ++++++++++++++++++++\n",
            "{'Recall@20': 0.021899224806201552, 'NDCG@20': tensor(0.0086, device='cuda:0'), 'Recall@10': 0.012936046511627906, 'NDCG@10': tensor(0.0064, device='cuda:0'), 'Recall@5': 0.0075096899224806205, 'NDCG@5': tensor(0.0047, device='cuda:0'), 'Recall@1': 0.0020348837209302325, 'NDCG@1': tensor(0.0020, device='cuda:0')}\n"
          ]
        }
      ],
      "source": [
        "# Load the best-performing model weights from the saved checkpoint file.\n",
        "\n",
        "# The 'best_weight' variable is used to load the model weights from a specified checkpoint file. The file path is constructed based on the provided configuration and hyperparameters (args) to ensure the correct checkpoint file is loaded.\n",
        "\n",
        "best_weight2 = torch.load(os.path.join(args2.save_path, f'{args2.model_name}_seed{args2.seed}_is_pretrain_{args2.is_pretrain}_best_model_lr{args2.lr}_wd{args2.weight_decay}_block{args2.block_num}_hd{args2.hidden_size}_emb{args2.embedding_size}.pth'))\n",
        "\n",
        "# Load the model's state dictionary with the loaded best weights.\n",
        "\n",
        "# The model's state dictionary is updated with the best weights, effectively loading the best-performing model configuration.\n",
        "\n",
        "model2.load_state_dict(best_weight2)\n",
        "\n",
        "# Move the model to the specified device (GPU or CPU).\n",
        "\n",
        "# The 'model' is moved to the device specified in args.device, allowing computations and predictions to be performed on the chosen device.\n",
        "\n",
        "model2 = model2.to(args2.device)\n",
        "\n",
        "# Perform full sequence validation on the test dataset.\n",
        "\n",
        "# The Sequence_full_Validate function is called to evaluate the model's performance on the test dataset (test_dataloader).\n",
        "# The function takes the provided model, performs sequence validation, and logs relevant evaluation metrics using the specified SummaryWriter (writer) for TensorBoard.\n",
        "\n",
        "metrics2 = trainer.Sequence_full_Validate(0, model2, test_dataloader2, writer, args2, test=False)\n",
        "\n",
        "# Close the SummaryWriter.\n",
        "\n",
        "# The SummaryWriter is closed to finish logging the training and evaluation metrics for TensorBoard visualization and monitoring.\n",
        "\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Insights, Observations, and Conclusion:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment 2 Results:\n",
        "\n",
        "### Training Loss and Validation Metrics:\n",
        "\n",
        "- During the training process, the Cross-Entropy (CE) loss decreases with each epoch, which is a positive sign as it indicates that the model is effectively learning from the training data.\n",
        "\n",
        "- However, the evaluation metrics (recall and NDCG) on the validation set remain consistently low and show no improvement throughout the training process. This indicates that the model is not able to generalize well to unseen data, leading to slightly good recommendation performance.\n",
        "\n",
        "### Model Architecture:\n",
        "\n",
        "The model architecture consists of an item embedding layer, followed by multiple residual blocks. The model then has a final linear layer for output.\n",
        "\n",
        "### Evaluation Results:\n",
        "\n",
        "The evaluation results on the validation set indicate good recommendation performance compared to the leaderboard here: https://tenrec0.github.io/. The NDCG@20 metric are consistently near to the leaderboard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- We trained this model on different dataset of users group where age = 0.\n",
        "- Despite the model learning from the training data (as evidenced by decreasing training loss), it generalized slightly good, leading to good recommendation performance.\n",
        "- The suboptimal results could be attributed to overfitting, inappropriate hyperparameters, or data preprocessing issues.\n",
        "- Further experimentation, including hyperparameter tuning and regularization is necessary to improve the model's recommendation capabilities."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
